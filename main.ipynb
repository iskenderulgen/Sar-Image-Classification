{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.classification import (\n",
    "    NaiveBayes,\n",
    "    DecisionTreeClassifier,\n",
    "    RandomForestClassifier,\n",
    "    LogisticRegression,\n",
    ")\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml import Pipeline\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To simulate a cluster environment, change the instance size to test multi instance performance\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.master(\"local\")\n",
    "    .appName(\"Sar_Image_Analysis\")\n",
    "    .config(\"spark.driver.cores\", \"2\")\n",
    "    .config(\"spark.driver.memory\", \"2g\")\n",
    "    .config(\"spark.executor.cores\", \"2\")\n",
    "    .config(\"spark.executor.memory\", \"2g\")\n",
    "    .config(\"spark.driver.maxResultSize\", \"3g\")\n",
    "    .config(\"spark.executor.instances\", \"4\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Training Data along with its labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "# This assumes that all datasets have the same schema\n",
    "# Load the datasets\n",
    "\n",
    "alongside = spark.read.csv(\"data/alongside.csv\").withColumn(\"string_label\", F.lit(\"alongside\"))\n",
    "building = spark.read.csv(\"data/building.csv\").withColumn(\"string_label\", F.lit(\"building\"))\n",
    "road = spark.read.csv(\"data/road.csv\").withColumn(\"string_label\", F.lit(\"road\"))\n",
    "vegetation = spark.read.csv(\"data/vegetation.csv\").withColumn(\"string_label\", F.lit(\"vegetation\"))\n",
    "water = spark.read.csv(\"data/water.csv\").withColumn(\"string_label\", F.lit(\"water\"))\n",
    "\n",
    "# Combine all datasets into one training dataset\n",
    "training_dataset = (\n",
    "    alongside\n",
    "    .union(building)\n",
    "    .union(road)\n",
    "    .union(vegetation)\n",
    "    .union(water)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Feature format to float and Assemble Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convert all feature columns to FloatType This is necessary for MLlib to work with the data.\n",
    "We assume that all columns except the last one are features The last column is the label.\n",
    "\"\"\"\n",
    "\n",
    "# Identify all feature columns (all columns except the last one which is the label)\n",
    "feature_columns = training_dataset.columns[:-1]  # All columns except the last one\n",
    "\n",
    "# Cast multiple columns at once\n",
    "training_dataset = training_dataset.select(\n",
    "    *[F.col(c).cast(FloatType()).alias(c) for c in feature_columns],\n",
    "    F.col(\"string_label\")  # Keep label column as is\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature vector using VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "training_dataset = assembler.transform(training_dataset).select(\"features\", \"string_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String indexing for the label column\n",
    "string_indexer = StringIndexer(inputCol=\"string_label\", outputCol=\"label\", stringOrderType=\"alphabetAsc\")\n",
    "training_dataset = string_indexer.fit(training_dataset).transform(training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Split the dataset into training and test sets\n",
    "Note: In a real-world scenario, you would typically want to use a stratified split\n",
    "to ensure that each class is represented in both sets.\n",
    "For simplicity, we will use a random split here\n",
    "\"\"\"\n",
    "train, test = training_dataset.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# InÄ±talize classifier evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-Machine learning models for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1-Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes_model = NaiveBayes(\n",
    "    smoothing=0.01, modelType=\"multinomial\", featuresCol=\"features\", labelCol=\"label\"\n",
    ").fit(train)\n",
    "\n",
    "predictions = naive_bayes_model.transform(test)\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2-Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_model = RandomForestClassifier(\n",
    "    featuresCol=\"features\", labelCol=\"label\", numTrees=10, maxDepth=10\n",
    ").fit(train)\n",
    "\n",
    "predictions = random_forest_model.transform(test)\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3-Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_model = DecisionTreeClassifier(\n",
    "    featuresCol=\"features\", labelCol=\"label\", maxDepth=10\n",
    ").fit(train)\n",
    "\n",
    "predictions = decision_tree_model.transform(test)\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4-Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_model = LogisticRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    maxIter=10,\n",
    "    regParam=0.01,\n",
    "    elasticNetParam=0.8,\n",
    "    standardization=False,\n",
    ").fit(train)\n",
    "\n",
    "predictions = logistic_regression_model.transform(test)\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_summary = logistic_regression_model.summary\n",
    "\n",
    "metrics = {\n",
    "    \"Accuracy\": training_summary.accuracy,\n",
    "    \"FPR\": training_summary.weightedFalsePositiveRate,\n",
    "    \"TPR\": training_summary.weightedTruePositiveRate,\n",
    "    \"F-measure\": training_summary.weightedFMeasure(),\n",
    "    \"Precision\": training_summary.weightedPrecision,\n",
    "    \"Recall\": training_summary.weightedRecall,\n",
    "}\n",
    "\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-Cross Validation for all Models tested above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder().build()\n",
    "\n",
    "models = [\n",
    "    (\n",
    "        NaiveBayes(\n",
    "            smoothing=0.01,\n",
    "            modelType=\"multinomial\",\n",
    "            featuresCol=\"features\",\n",
    "            labelCol=\"label\",\n",
    "        ),\n",
    "        \"Naive Bayes\",\n",
    "    ),\n",
    "    (\n",
    "        RandomForestClassifier(\n",
    "            featuresCol=\"features\", labelCol=\"label\", numTrees=10, maxDepth=10\n",
    "        ),\n",
    "        \"Random Forest\",\n",
    "    ),\n",
    "    (\n",
    "        DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"label\", maxDepth=10),\n",
    "        \"Decision Tree\",\n",
    "    ),\n",
    "    (\n",
    "        LogisticRegression(\n",
    "            featuresCol=\"features\",\n",
    "            labelCol=\"label\",\n",
    "            maxIter=300,\n",
    "            regParam=0.01,\n",
    "            elasticNetParam=0.8,\n",
    "            standardization=False,\n",
    "        ),\n",
    "        \"Logistic Regression\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for model, model_name in models:\n",
    "    start = time.time()\n",
    "    cv = CrossValidator(\n",
    "        estimator=Pipeline(stages=[model]),\n",
    "        estimatorParamMaps=paramGrid,\n",
    "        evaluator=evaluator,\n",
    "        numFolds=10,\n",
    "        parallelism=10\n",
    "    )\n",
    "    \n",
    "    cvModel = cv.fit(training_dataset)\n",
    "    accuracy = evaluator.evaluate(cvModel.transform(training_dataset))\n",
    "    results.append((model_name, accuracy, time.time()-start))\n",
    "\n",
    "for name, accuracy, duration in results:\n",
    "    print(f\"{name}: Accuracy = {accuracy*100:.2f}% | Time = {duration:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-Run a trained model on full scale SAR image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use decision tree as the final model\n",
    "\n",
    "decision_tree_model = DecisionTreeClassifier(\n",
    "    featuresCol=\"features\", labelCol=\"label\", maxDepth=10\n",
    ").fit(training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the full scale SAR image in parquet format\n",
    "indiana_df = spark.read.parquet(\"data/indiana.parquet\")\n",
    "\n",
    "# Apply the same preprocessing steps as for the training data\n",
    "# Convert feature columns to FloatType\n",
    "indiana_df = indiana_df.select(*[F.col(c).cast(FloatType()).alias(c) for c in indiana_df.columns])\n",
    "\n",
    "# Assemble features into vector\n",
    "assembler = VectorAssembler(inputCols=indiana_df.columns, outputCol=\"features\")\n",
    "indiana_df = assembler.transform(indiana_df).select(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the trained decision tree model to classify the data\n",
    "indiana_predictions = decision_tree_model.transform(indiana_df)\n",
    "\n",
    "# Calculate total number of predictions to compute percentages\n",
    "total = indiana_predictions.count()\n",
    "\n",
    "# Group by the predicted string label and calculate count and percentage\n",
    "result = (\n",
    "    indiana_predictions.groupBy(\"prediction\")\n",
    "    .count()\n",
    "    .withColumn(\"percentage\", F.round(F.col(\"count\") * 100 / total))\n",
    ")\n",
    "\n",
    "result.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sar-image",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
